{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a66e5f-2d63-4c23-b741-b8b3ddbeca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('concern_corpus.csv', encoding ='latin1')  #load dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67582772-8748-44bd-ad6a-caf436211f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0,1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4138e1-8a40-4298-aeae-dbfe414f1b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Llama-2-7B-Chat-fp16\") # replace with other model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7B-Chat-fp16\", device_map=\"auto\") # replace with other model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92972c4f-f145-47b9-aa4f-1418246cddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######   Code for Prompts   ######\n",
    "\n",
    "##  One shot ##\n",
    "def prompt_engine(inp):\n",
    "    \"\"\"\n",
    "    This prompt wrapper is for One-shot prompting\n",
    "    inp: str : text from citation\n",
    "    output: str : prompt wrapper on citation\n",
    "\n",
    "    \"\"\"\n",
    "    Instruction = \"\"\"You are tasked with classifying citation sentences from scientific papers into one of two categories: Concerning Citation (CC) or Non-Concerning Citation (NC). Concerning Citation (CC) are sentences that raises issues related to the validity of the cited study. This may include concerns about the data, methodology, or other significant flaws that could undermine the overall reliability of the research.\n",
    "Non-Concerning Citation (NC) are sentences that may include a discussion or criticism of the cited study, but it does not question the validity of the study itself. These citations might provide neutral or contextual commentary or address limitations without undermining the core findings of the research.\"\"\"\n",
    "    system_prompt = f\"### System:\\n{Instruction}\\n\\n\"\n",
    "    example1 = \"\"\"After publication of this article [ |pone.0201274.ref001| ], concerns were raised about the scientific validity of the meta-analysis and whether it provided a rigorous and accurate assessment of published clinical studies on the efficacy of acupuncture or drug-based interventions for improving chronic constipation.\"\"\"\n",
    "    label1= \"CC\"\n",
    "    example2 = \"\"\"de Gouveia and Inglesi-Lotz (2021) observed a rapid increase of climate change-related records from both an absolute number and a relative share perspective. They also tried to give some possible explanations for the rise of research publications. However, more convincing reasons are needed to explain the abnormal growth of research publications for some specific years in various bibliometric analyses including the study conducted by de Gouveia and Inglesi-Lotz.\"\"\"\n",
    "    label2= \"NC\"\n",
    "    prompt = f\"{system_prompt}### User: {example1}\\n\\n### Assistant: {label1}\\n\\n### User: {example2}\\n\\n### Assistant: {label2}\\n\\n### User: {inp}\\n\\n### Assistant:\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "## Few Shot ##\n",
    "\n",
    "def prompt_engine_more_examples_2(inp):\n",
    "    \"\"\"\n",
    "    This prompt wrapper is for Few-shot prompting 2 examples\n",
    "    inp: str : text from citation\n",
    "    output: str : prompt wrapper on citation\n",
    "\n",
    "    \"\"\"\n",
    "    Instruction = \"\"\"You are tasked with classifying citation sentences from scientific papers into one of two categories: Concerning Citation (CC) or Non-Concerning Citation (NC). Concerning Citation (CC) are sentences that raises issues related to the validity of the cited study. This may include concerns about the data, methodology, or other significant flaws that could undermine the overall reliability of the research.\n",
    "Non-Concerning Citation (NC) are sentences that may include a discussion or criticism of the cited study, but it does not question the validity of the study itself. These citations might provide neutral or contextual commentary or address limitations without undermining the core findings of the research.\"\"\"\n",
    "    system_prompt = f\"### System:\\n{Instruction}\\n\\n\"\n",
    "    example1 = \"\"\"After publication of this article [ |pone.0201274.ref001| ], concerns were raised about the scientific validity of the meta-analysis and whether it provided a rigorous and accurate assessment of published clinical studies on the efficacy of acupuncture or drug-based interventions for improving chronic constipation.\"\"\"\n",
    "    label1= \"CC\"\n",
    "    example2 = \"\"\"de Gouveia and Inglesi-Lotz (2021) observed a rapid increase of climate change-related records from both an absolute number and a relative share perspective. They also tried to give some possible explanations for the rise of research publications. However, more convincing reasons are needed to explain the abnormal growth of research publications for some specific years in various bibliometric analyses including the study conducted by de Gouveia and Inglesi-Lotz.\"\"\"\n",
    "    label2= \"NC\"\n",
    "    \n",
    "    \n",
    "    example3 = '''Such resources exist for Hebrew ( ), but unfortunately use a tagging scheme which is incompatible with the one of the Hebrew Treebank.'''\n",
    "    label3= \"NC\"\n",
    "    example4 = '''However, it is not clear that AER as defined by Och and Ney ( 2003 ) is always the appropriate way to evaluate the quality of the model, since the Viterbi word alignment that AER is based on is seldom used in applications of Model 1.5. Moreover, it is notable that while the versions of Model 1 having the lowest AER have dramatically higher precision than the standard version, they also have quite a bit lower recall.'''\n",
    "    label4= \"CC\"\n",
    "    \n",
    "    prompt = f\"{system_prompt}### User: {example1}\\n\\n### Assistant: {label1}\\n\\n### User: {example2}\\n\\n### Assistant: {label2}\\n\\n### User: {example3}\\n\\n### Assistant: {label3}\\n\\n### User: {example4}\\n\\n### Assistant: {label4}\\n\\n### User: {inp}\\n\\n### Assistant:\"\n",
    "    return prompt\n",
    "\n",
    "def prompt_engine_more_examples_3(inp):\n",
    "    \"\"\"\n",
    "    This prompt wrapper is for Few-shot prompting 3 examples\n",
    "    inp: str : text from citation\n",
    "    output: str : prompt wrapper on citation\n",
    "\n",
    "    \"\"\"\n",
    "    Instruction = \"\"\"You are tasked with classifying citation sentences from scientific papers into one of two categories: Concerning Citation (CC) or Non-Concerning Citation (NC). Concerning Citation (CC) are sentences that raises issues related to the validity of the cited study. This may include concerns about the data, methodology, or other significant flaws that could undermine the overall reliability of the research.\n",
    "Non-Concerning Citation (NC) are sentences that may include a discussion or criticism of the cited study, but it does not question the validity of the study itself. These citations might provide neutral or contextual commentary or address limitations without undermining the core findings of the research.\"\"\"\n",
    "    system_prompt = f\"### System:\\n{Instruction}\\n\\n\"\n",
    "    example1 = \"\"\"After publication of this article [ |pone.0201274.ref001| ], concerns were raised about the scientific validity of the meta-analysis and whether it provided a rigorous and accurate assessment of published clinical studies on the efficacy of acupuncture or drug-based interventions for improving chronic constipation.\"\"\"\n",
    "    label1= \"CC\"\n",
    "    example2 = \"\"\"de Gouveia and Inglesi-Lotz (2021) observed a rapid increase of climate change-related records from both an absolute number and a relative share perspective. They also tried to give some possible explanations for the rise of research publications. However, more convincing reasons are needed to explain the abnormal growth of research publications for some specific years in various bibliometric analyses including the study conducted by de Gouveia and Inglesi-Lotz.\"\"\"\n",
    "    label2= \"NC\"\n",
    "    \n",
    "    example3 = '''Such resources exist for Hebrew ( ), but unfortunately use a tagging scheme which is incompatible with the one of the Hebrew Treebank.'''\n",
    "    label3 = 'NC'\n",
    "    example4= '''At the same time, we believe our method has advantages over the approach developed initially at IBM (Brown et al. 1990; Brown et al. 1993) for training translation systems automatically.'''\n",
    "    label4 = 'NC'\n",
    "\n",
    "    example5= '''However, it is not clear that AER as defined by Och and Ney ( 2003 ) is always the appropriate way to evaluate the quality of the model, since the Viterbi word alignment that AER is based on is seldom used in applications of Model 1.5. Moreover, it is notable that while the versions of Model 1 having the lowest AER have dramatically higher precision than the standard version, they also have quite a bit lower recall .'''\n",
    "    label5 = 'CC'\n",
    "    example6 = '''However, Koskenniemi himself understood that his initial implementation had significant limitations in handling non-concatenative morphotactic processes.'''\n",
    "    label6 = 'CC'\n",
    "\n",
    "    \n",
    "    \n",
    "    prompt = f\"{system_prompt}### User: {example1}\\n\\n### Assistant: {label1}\\n\\n### User: {example2}\\n\\n### Assistant: {label2}\\n\\n### User: {example3}\\n\\n### Assistant: {label3}\\n\\n### User: {example4}\\n\\n### Assistant: {label4}\\n\\n### User: {example5}\\n\\n### Assistant: {label5}\\n\\n### User: {example6}\\n\\n### Assistant: {label6}\\n\\n### User: {inp}\\n\\n### Assistant:\"\n",
    "    return prompt\n",
    "\n",
    "def prompt_engine_more_examples_4(inp):\n",
    "    \"\"\"\n",
    "    This prompt wrapper is for Few-shot prompting 4 examples http://127.0.0.1:8888/tree?token=506c59e0ccaf94577f3182e8bf4ff76d9e183b329e5dc5a5\n",
    "\n",
    "    inp: str : text from citation\n",
    "    output: str : prompt wrapper on citation\n",
    "\n",
    "    \"\"\"\n",
    "    Instruction = \"\"\"You are tasked with classifying citation sentences from scientific papers into one of two categories: Concerning Citation (CC) or Non-Concerning Citation (NC). Concerning Citation (CC) are sentences that raises issues related to the validity of the cited study. This may include concerns about the data, methodology, or other significant flaws that could undermine the overall reliability of the research.\n",
    "Non-Concerning Citation (NC) are sentences that may include a discussion or criticism of the cited study, but it does not question the validity of the study itself. These citations might provide neutral or contextual commentary or address limitations without undermining the core findings of the research.\"\"\"\n",
    "    system_prompt = f\"### System:\\n{Instruction}\\n\\n\"\n",
    "    example1 = \"\"\"After publication of this article [ |pone.0201274.ref001| ], concerns were raised about the scientific validity of the meta-analysis and whether it provided a rigorous and accurate assessment of published clinical studies on the efficacy of acupuncture or drug-based interventions for improving chronic constipation.\"\"\"\n",
    "    label1= \"CC\"\n",
    "    example2 = \"\"\"de Gouveia and Inglesi-Lotz (2021) observed a rapid increase of climate change-related records from both an absolute number and a relative share perspective. They also tried to give some possible explanations for the rise of research publications. However, more convincing reasons are needed to explain the abnormal growth of research publications for some specific years in various bibliometric analyses including the study conducted by de Gouveia and Inglesi-Lotz.\"\"\"\n",
    "    label2= \"NC\"\n",
    "    \n",
    "    example3 = '''Such resources exist for Hebrew ( ), but unfortunately use a tagging scheme which is incompatible with the one of the Hebrew Treebank.'''\n",
    "    label3 = 'NC'\n",
    "    example4= '''At the same time, we believe our method has advantages over the approach developed initially at IBM (Brown et al. 1990; Brown et al. 1993) for training translation systems automatically.'''\n",
    "    label4 = 'NC'\n",
    "\n",
    "    example5= '''However, it is not clear that AER as defined by Och and Ney ( 2003 ) is always the appropriate way to evaluate the quality of the model, since the Viterbi word alignment that AER is based on is seldom used in applications of Model 1.5. Moreover, it is notable that while the versions of Model 1 having the lowest AER have dramatically higher precision than the standard version, they also have quite a bit lower recall .'''\n",
    "    label5 = 'CC'\n",
    "    example6 = '''However, Koskenniemi himself understood that his initial implementation had significant limitations in handling non-concatenative morphotactic processes.'''\n",
    "    label6 = 'CC'\n",
    "\n",
    "    example7 = '''Most recently, (Suzuki and Isozaki, 2008) published their Semi-supervised sequential labelling method, whose results on POS tagging seem to be optically better than (Shen et al., 2007), but no significance tests were given and the tool is not available for download, i.e. for repeating the results and significance testing.'''\n",
    "    label7 = 'CC'\n",
    "    example8= '''(Hu and Liu, 2004) doesn't assess candidate features, so its precision is lower than opine's. (Kobayashi et al., 2004) employs an iterative semi-automatic approach which requires human input at every iteration. Neither model explicitly addresses composite (feature of feature) or implicit features.'''\n",
    "    label8 = 'NC'\n",
    "    \n",
    "    prompt = f\"{system_prompt}### User: {example1}\\n\\n### Assistant: {label1}\\n\\n### User: {example2}\\n\\n### Assistant: {label2}\\n\\n### User: {example3}\\n\\n### Assistant: {label3}\\n\\n### User: {example4}\\n\\n### Assistant: {label4}\\n\\n### User: {example5}\\n\\n### Assistant: {label5}\\n\\n### User: {example6}\\n\\n### Assistant: {label6}\\n\\n### User: {example7}\\n\\n### Assistant: {label7}\\n\\n### User: {example8}\\n\\n### Assistant: {label8}\\n\\n### User: {inp}\\n\\n### Assistant:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc45e2-ee1d-4150-8040-7a23d1bf2102",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Code for processing the model outputs into meaningful representation ###\n",
    "\n",
    "def process_output_oneshot(output, current_data):\n",
    "    \"\"\"\n",
    "    output: Output of the model when using One-shot prompting\n",
    "    \"\"\"\n",
    "    out = output.split(current_data[-6:])\n",
    "    label = out[-1].split('\\n\\n### Assistant:')\n",
    "    label= label[1][:4]\n",
    "    if 'CC' in label:\n",
    "        return 'CC'\n",
    "    if 'NC' in label:\n",
    "        return 'NC'\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def process_output_fewshot(output):\n",
    "    \"\"\"\n",
    "    output: Output of the model when using Few-shot prompting\n",
    "    \"\"\"\n",
    "    out = output.split('### Assistant:')\n",
    "    label = out[-1]\n",
    "    if 'CC' in label:\n",
    "        return 'CC'\n",
    "    if 'NC' in label:\n",
    "        return 'NC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4739a629-e93c-44d0-990d-b673e4d28a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a2ec08-248d-4ae7-a262-8cc1368fbf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####  Code for making the generation ####\n",
    "\n",
    "\n",
    "##  Oneshot ##\n",
    "from tqdm import tqdm\n",
    "generations = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    index = index \n",
    "    label = row['label'] \n",
    "    citation = row['citation']\n",
    "    ## prompt\n",
    "    prompt = prompt_engine(citation)\n",
    "    ##  run generation ## \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "    output = model.generate(**inputs, do_sample=False, max_new_tokens= 10)\n",
    "    \n",
    "    out = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    ## process predicted label\n",
    "    predicted_label = process_output(out, citation) \n",
    "    store = (index, citation, ('Actual_label', label), ('Predicted_label', predicted_label))\n",
    "    generations.append(store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51b6788-b46b-44dc-8815-59395c81f2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Few shot ##\n",
    "generations = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    index = index \n",
    "    label = row['label'] \n",
    "    citation = row['citation']\n",
    "    ## prompt\n",
    "    prompt = prompt_engine_more_examples_2(citation)  # chose appropriate prompt from fewshot prompt functions\n",
    "    ##  run generation ## \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "    output = model.generate(**inputs, do_sample=False, max_new_tokens= 10)\n",
    "    \n",
    "    out = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    ## process predicted label\n",
    "    predicted_label = process_output_fewshot(out) \n",
    "    store = (index, citation, ('Actual_label', label), ('Predicted_label', predicted_label))\n",
    "    generations.append(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cc0197-0fb1-4bc8-b487-24f3e46c10ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
